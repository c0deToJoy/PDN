ITERATION 17: tuned\_variant13\_op\_mr\_ilp
================================================================================

1. CODE CHANGES FROM PREVIOUS ITERATION
--------------------------------------------------------------------------------
Unrolls the m-dimension (MR) loop in the micro-kernel by adding `\#pragma unroll BLOCK\_MR` directive. This exposes all BLOCK\_MR×BLOCK\_NR operations in the innermost loop for compiler optimization:

```c
// Initialize C\_micro with MR-unrolled loop
for( int j0\_r = 0; j0\_r < BLOCK\_NR; ++j0\_r  )
  \#pragma unroll BLOCK\_MR
  for( int i0\_r = 0; i0\_r < BLOCK\_MR; ++i0\_r  )
    {
      C\_micro[j0\_r][i0\_r] = 0.0f;
    }

// Main computation with k and MR unrolling
for( int p0\_i = 0; p0\_i < block\_kc\_remainder; p0\_i += BLOCK\_KU )
  \#pragma unroll BLOCK\_KU
  for( int p0\_u = 0; p0\_u < BLOCK\_KU; ++ p0\_u )
    for( int j0\_r = 0; j0\_r < BLOCK\_NR; ++j0\_r  )
      \#pragma unroll BLOCK\_MR
      for( int i0\_r = 0; i0\_r < BLOCK\_MR; ++i0\_r  )
      {
        int j0 = j0\_o + j0\_i + j0\_r;
        int i0 = i0\_o + i0\_i + i0\_r;
        int p0 = p0\_o + p0\_i + p0\_u;
        
        int j0\_i\_bid = j0\_i/(BLOCK\_NR);
        int i0\_i\_bid = i0\_i/(BLOCK\_MR);
        
        float A\_ip = A\_dlt[i0\_i\_bid][p0\_i+p0\_u][i0\_r];
        float B\_pj = B\_dlt[j0\_i\_bid][p0\_i+p0\_u][j0\_r];
        
        C\_micro[j0\_r][i0\_r] += A\_ip*B\_pj;
      }

// Write-back with MR unrolling
for( int j0\_r = 0; j0\_r < BLOCK\_NR; ++j0\_r  )
  \#pragma unroll BLOCK\_MR
  for( int i0\_r = 0; i0\_r < BLOCK\_MR; ++i0\_r  )
    {
      int j0 = j0\_o + j0\_i + j0\_r;
      int i0 = i0\_o + i0\_i + i0\_r;
      C\_distributed[i0 * cs\_C + j0 * rs\_C] += C\_micro[j0\_r][i0\_r];
    }
```

This unrolls all BLOCK\_MR iterations in the i0\_r loop, exposing independent FMA operations.

2. PERFORMANCE ANALYSIS
--------------------------------------------------------------------------------
Performance data from CSV files:

Variant 12 (iteration 16): 128x128x128 = 5.99 GFLOPs
Variant 13 (iteration 17): 128x128x128 = 7.08 GFLOPs (+18.2\% improvement)

Corresponding plot: results/plot\_iter\_17\_tuned\_variant13\_op\_mr\_ilp.png


3. EXPLANATION AND HYPOTHESIS
--------------------------------------------------------------------------------
**Mixed Performance Results**: CSV data reveals size-dependent behavior. At 128x128x128: +18.2% gain (5.99 \textrightarrow 7.08 GFLOPs). However, at smaller sizes (16-96) iteration 17 shows **marginal or negative** change, and at larger sizes (176+) both variants converge to 7.01 GFLOPs (iteration 17 slightly worse or equal).

Hypothesis for size-dependent behavior:
1. **Smaller matrices (16-96)**: MR unrolling increases register pressure without sufficient ILP window, causing degradation relative to iteration 16's conservative k-only unrolling
2. **Sweet spot (112-128)**: MR unrolling benefits from optimal balance of register availability and instruction scheduling window, providing 18% improvement
3. **Larger matrices (144+)**: Both variants converge as memory bandwidth becomes bottleneck; unrolling overhead dominates, causing iteration 17 to regress or match iteration 16

The MR-dimension loop unrolling with `\#pragma unroll BLOCK\_MR` exposes BLOCK\_MR (16) independent accumulator accesses per j0\_r iteration, allowing better compiler scheduling on problems where the micro-kernel execution window aligns with CPU superscalar capabilities. Combined with k-dimension unrolling (BLOCK\_KU=8), this creates 16×8 = 128 instruction-level parallelism opportunities.

However, this approach has fundamental limitations:
1. Register pressure increases significantly with both k and MR unrolling—larger matrices suffer from spilling
2. Underlying baseline still 61.8\% below iteration 13 (20.14 GFLOPs) due to broken n-fringe minimization from iteration 14
3. Without SIMD vectorization, scalar FMA latency (4-5 cycles) limits ILP effectiveness
4. Benefit is problem-size dependent; not universally applicable

To realize sustainable improvements:
- MR unrolling helps only in specific size ranges; regression on larger problems suggests register overflow
- SIMD vectorization (next iteration) is essential to overcome both register pressure and scalar FMA latency
- Consider dynamic unrolling strategies or problem-size-aware compilation
