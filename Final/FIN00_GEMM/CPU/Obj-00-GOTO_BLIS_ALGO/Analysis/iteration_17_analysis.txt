ITERATION 17: tuned_variant13_op_mr_ilp
================================================================================

1. CODE CHANGES FROM PREVIOUS ITERATION
--------------------------------------------------------------------------------
Unrolls the m-dimension (MR) loop in the micro-kernel to improve instruction-level parallelism:

```c
// Explicitly unroll all MR rows
c00 += a0 * b0; c01 += a0 * b1; c02 += a0 * b2; c03 += a0 * b3;
c10 += a1 * b0; c11 += a1 * b1; c12 += a1 * b2; c13 += a1 * b3;
c20 += a2 * b0; c21 += a2 * b1; c22 += a2 * b2; c23 += a2 * b3;
c30 += a3 * b0; c31 += a3 * b1; c32 += a3 * b2; c33 += a3 * b3;
```

This exposes all MRÃ—NR operations to the compiler for better scheduling.

2. PERFORMANCE ANALYSIS
--------------------------------------------------------------------------------
Performance data from CSV files:

Performance data not available in CSV format

Corresponding plot: results/plot_iter_17_tuned_variant13_op_mr_ilp.png


3. EXPLANATION AND HYPOTHESIS
--------------------------------------------------------------------------------
Instruction-level parallelism (ILP) optimization through loop unrolling
allows multiple independent operations to execute simultaneously on modern
superscalar processors, improving throughput.

To test this hypothesis:
- Profile cache miss rates (perf stat -e cache-misses,cache-references)
- Analyze instruction mix and IPC (instructions per cycle)
- Vary block sizes to find optimal values for target architecture
- Compare performance across different matrix sizes and shapes
