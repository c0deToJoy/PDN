ITERATION 24: tuned\_variant20\_op\_2d\_sharedmem
================================================================================

1. CODE CHANGES FROM PREVIOUS ITERATION
--------------------------------------------------------------------------------
Extends 1D OpenMP parallelism to 2D work distribution with nested pragma directives. The outer loop distributes NC-blocks across PAR\_COL\_THREADS threads, and the inner loop distributes MC-blocks across PAR\_ROW\_THREADS threads. Each thread now works on an (MC x NC x KC) sub-block; the nested OpenMP pragmas distribute work across both N (columns) and M (rows) dimensions, improving load balancing and reducing thread idle time.

```c
// 2D shared-memory parallelism: NC x MC decomposition
int num\_j0\_i\_blocks = (BLOCK\_NC) / (BLOCK\_NR);
int num\_i0\_i\_blocks = (BLOCK\_MC) / (BLOCK\_MR);

// Outer level: distribute NC-blocks across column threads
\#pragma omp parallel for num\_threads(PAR\_COL\_THREADS)
for( int j0\_o = 0; j0\_o < n0\_fringe\_start; j0\_o += BLOCK\_NC )
  {
    int block\_nc\_remainder = min(BLOCK\_NC, n0\_fringe\_start-j0\_o);
    
    for( int p0\_o = 0; p0\_o < k0; p0\_o += BLOCK\_KC )
      {
        // Pack B (shared within column thread)
        float B\_dlt[num\_j0\_i\_blocks][BLOCK\_KC][BLOCK\_NR];
        
        // Inner level: distribute MC-blocks across row threads
        \#pragma omp parallel for num\_threads(PAR\_ROW\_THREADS)
        for( int i0\_o = 0; i0\_o < m0\_fringe\_start; i0\_o += BLOCK\_MC )
          {
            // Pack A (private to row thread)
            float A\_dlt[num\_i0\_i\_blocks][BLOCK\_KC][BLOCK\_MR/PAR\_VECT\_LEN][PAR\_VECT\_LEN];
            
            // Micro-kernel computation with SIMD
            // Each (row\_thread, col\_thread) pair computes independent MC x NC x KC block
          }
      }
  }
```

2. PERFORMANCE ANALYSIS
--------------------------------------------------------------------------------
Variant 19 (iteration 23 - 1D OpenMP parallel): 128x128x128 = 47.16 GFLOPs
Variant 20 (iteration 24 - 2D OpenMP parallel): 128x128x128 = 58.29 GFLOPs (+23.6% improvement)

Corresponding plot: results/plot_iter_24_tuned_variant20_op_2d_sharedmem.png


3. EXPLANATION AND HYPOTHESIS
--------------------------------------------------------------------------------
**Size-Dependent Performance**: CSV data reveals mixed results across problem sizes. At 128×128×128: +23.6% improvement (47.16 \textrightarrow 58.29 GFLOPs). However, at smaller sizes (16-112) and some larger sizes (144), iteration 24 performs **worse** than iteration 23. There is only optimal alignment between 2D thread decomposition and cache architecture at mid-range problem sizes, while other sizes suffer from increased nested parallelism overhead (two levels of barriers, thread team creation) without corresponding load balance benefits. This demonstrates 2D parallelism's sensitivity to the relationship between problem size, block dimensions, and thread count.
