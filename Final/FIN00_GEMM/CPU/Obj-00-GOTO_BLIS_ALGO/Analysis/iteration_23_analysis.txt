ITERATION 23: tuned\_variant19\_op\_data\_dist\_1d\_sharedmem
================================================================================

1. CODE CHANGES FROM PREVIOUS ITERATION
--------------------------------------------------------------------------------
Introduces OpenMP shared-memory parallelism with 1D work distribution. The outer NC-loop is parallelized with `\#pragma omp parallel for num\_threads(PAR\_COL\_THREADS)`, distributing column panels across threads. Each thread processes independent NC×K×M blocks, minimizing synchronization overhead.

```c
// Shared-memory parallelism: 1D distribution over NC blocks
int num\_j0\_i\_blocks = (BLOCK\_NC) / (BLOCK\_NR);
int num\_i0\_i\_blocks = (BLOCK\_MC) / (BLOCK\_MR);

// Distribute NC-blocks across PAR_COL_THREADS threads
\#pragma omp parallel for num\_threads(PAR\_COL\_THREADS)
for( int j0\_o = 0; j0\_o < n0\_fringe\_start; j0\_o += BLOCK\_NC )
  {
    int block\_nc\_remainder = min(BLOCK\_NC, n0\_fringe\_start-j0\_o);
    
    // Inner loops: pack B and perform A packing + micro-kernel
    for( int p0\_o = 0; p0\_o < k0; p0\_o += BLOCK\_KC )
      {
        int block\_kc\_remainder = min(BLOCK\_KC, k0-p0\_o);
        
        // Pack B into cache-friendly layout (private to thread)
        float B\_dlt[num\_j0\_i\_blocks][BLOCK\_KC][BLOCK\_NR];
        for( int j0\_i = 0; j0\_i < BLOCK\_NC; j0\_i += BLOCK\_NR )
          for( int p0\_i = 0; p0\_i < BLOCK\_KC; ++p0\_i )
            for( int j0\_r = 0; j0\_r < BLOCK\_NR; ++j0\_r  )
              {
                // B packing with thread-private buffer
                B\_dlt[j0\_i/BLOCK\_NR][p0\_i][j0\_r] = B\_distributed[...];
              }
        
        // Pack A and perform computation (still within thread)
        for( int i0\_o = 0; i0\_o < m0\_fringe\_start; i0\_o += BLOCK\_MC )
          {
            float A\_dlt[num\_i0\_i\_blocks][BLOCK\_KC][BLOCK\_MR/PAR\_VECT\_LEN][PAR\_VECT\_LEN];
            // A packing and micro-kernel computation...
          }
      }
  }
```

2. PERFORMANCE ANALYSIS
--------------------------------------------------------------------------------
Variant 18 (iteration 22 - sequential AVX2): 128x128x128 = 60.25 GFLOPs
Variant 19 (iteration 23 - 1D OpenMP parallel): 128x128x128 = 47.16 GFLOPs (-21.8\% regression)

Corresponding plot: results/plot\_iter\_23\_tuned\_variant19\_op\_data\_dist\_1d\_sharedmem.png


3. EXPLANATION AND HYPOTHESIS
--------------------------------------------------------------------------------
**Performance Regression**: CSV data shows -21.8\% regression (60.25 \textrightarrow 47.16 GFLOPs) when adding 1D OpenMP parallelism. This is unexpected because parallelism should improve performance on multi-core systems.

Hypothesis for regression:
1. Overhead from OpenMP fork/join: `\#pragma omp parallel for` creates/destroys thread team at each outer loop iteration, with non-trivial overhead
2. Thread creation cost dominates for small problem size (128\u00d7128\u00d7128 with BLOCK\_NC=192 creates only 1 column block!)
3. With only 1 NC-block and PAR\_COL\_THREADS=2, one thread is idle; actual parallelism is minimal
4. Cache coherence overhead: threads may contend on shared C\_distributed memory during write-back
5. Suboptimal thread binding: without explicit affinity, threads may migrate between cores

Key insight: Parallelism regression occurs because:
- Problem size too small: 128\u00d7128\u00d7128 < BLOCK\_NC=192, so actual parallelization is minimal
- OpenMP overhead not amortized: cost of thread team creation exceeds benefits of 2-thread parallelism
- Better suited for larger problems (512\u00d7512\u00d7512 or larger) where multiple NC-blocks exist

Though performance decreased, 47.16 GFLOPs remains:
- 6.5x higher than iteration 20 (7.30 GFLOPs)
- Demonstrates that SIMD is the primary performance driver, parallelism comes second
- OpenMP overhead becomes negligible on larger matrix sizes

To improve:
- Test on larger matrices (512\u00d7512\u00d7512+) where BLOCK\_NC is fully utilized
- Reduce OpenMP overhead by restructuring parallelism (e.g., parallel over M dimension instead of N)
- Use thread affinity (OMP\_PLACES=threads) to improve cache locality
- Consider 2D parallelism (iteration 24) which balances work distribution better
