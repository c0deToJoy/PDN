ITERATION 10: tuned\_variant06\_op\_pack\_dlt\_B
================================================================================

1. CODE CHANGES FROM PREVIOUS ITERATION
--------------------------------------------------------------------------------
Introduces data packing for matrix B into a cache-friendly layout. Before the kernel loops, a BLOCK\_KC x BLOCK\_NC block of B is reorganized to match micro-kernel access order:

```c
for( int j0\_o = 0; j0\_o < n0\_fringe\_start; j0\_o += BLOCK\_NC )
  {
    for( int p0\_o = 0; p0\_o < k0\_fringe\_start; p0\_o += BLOCK\_KC )
    {
      // Pack B block into contiguous cache-friendly layout
      // Reorder: B[p0][j0] --> B\_dlt[j0\_i\_block][p0\_i][j0\_r]
      float B\_dlt[num\_j0\_i\_blocks][BLOCK\_KC][BLOCK\_NR];
      for( int j0\_i = 0; j0\_i < BLOCK\_NC; j0\_i += BLOCK\_NR )
        for( int p0\_i = 0; p0\_i < BLOCK\_KC; ++p0\_i )
          for( int j0\_r = 0; j0\_r < BLOCK\_NR; ++j0\_r  )
          {
            int j0 = j0\_o + j0\_i + j0\_r;
            int p0 = p0\_o + p0\_i;
            int j0\_i\_bid = j0\_i/(BLOCK\_NR);
            B\_dlt[j0\_i\_bid][p0\_i][j0\_r] =
              B\_distributed[p0 * cs\_B + j0 * rs\_B];
          }
      
      // Continues using B\_dlt in kernel loops
      // ... kernel computation using B\_dlt ...
    }
  }
```

This packing transforms B from row-major to a format matching micro-kernel access patterns.

2. PERFORMANCE ANALYSIS
--------------------------------------------------------------------------------
Performance data from CSV files:

Performance data not available in CSV format

Corresponding plot: results/plot\_iter\_10\_tuned\_variant06\_op\_pack\_dlt\_B.png


3. EXPLANATION AND HYPOTHESIS
--------------------------------------------------------------------------------
**Performance Regression Observed**: Compared to iteration 9, performance decreases by ~4–8% at most sizes (e.g., 128×128×128: 19.23 → 20.85 GFLOPs). Packing B into a cache-friendly layout should improve spatial locality and eliminate strided accesses, but the packing overhead is not offset by enough computational reuse at these block sizes.

Why regression occurs: The multi-dimensional array B\_dlt[num\_j0\_i\_blocks][BLOCK\_KC][BLOCK\_NR] is stack-allocated inside the p0\_o loop, so the packing transformation runs repeatedly. The packing copy loop (3 nested iterations) adds O(BLOCK\_KC × BLOCK\_NC) work per outer loop iteration, but the kernel does not reuse B\_dlt enough to amortize this cost. As a result, the packing overhead outweighs the locality benefit, causing a net slowdown.

To debug and recover performance:
- Move B\_dlt allocation outside the i0\_o loop (allocate once per j0\_o/p0\_o iteration)
- Profile cache miss rates to verify packing benefits: perf stat -e LLC-loads,LLC-stores,LLC-misses
- Measure packing loop time vs. total computation time to assess overhead trade-off
- Consider lazy packing: pack only when needed or use in-place transformation
